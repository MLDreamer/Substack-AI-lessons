{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi9WPNJQ/r5Yhfqe7Q/IPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLDreamer/Substack-AI-lessons/blob/main/RAG_Evaluation_The_Faithfulness_Recall_Frontier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63zwaWjRS6vV"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RAG Evaluation: The Faithfulness-Recall Frontier\n",
        "==================================================\n",
        "\n",
        "This notebook demonstrates why standard RAG metrics (ROUGE, BLEU, Answer F1) fail\n",
        "and how Faithfulness and Context Recall expose the fundamental trade-off.\n",
        "\n",
        "Based on real production experience building a pharmaceutical supply chain chatbot.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: Environment Setup\n",
        "# ==============================================================================\n",
        "\n",
        "# Install required packages\n",
        "# Uncomment and run if packages are missing:\n",
        "# !pip install -q openai anthropic chromadb sentence-transformers pandas numpy matplotlib\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Vector store\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "print(\"âœ“ Environment setup complete\")\n",
        "print(\"\\nConfigure your API key:\")\n",
        "print(\"os.environ['OPENAI_API_KEY'] = 'your-key-here'\")\n",
        "print(\"# or\")\n",
        "print(\"os.environ['ANTHROPIC_API_KEY'] = 'your-key-here'\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: Dataset - Real Examples from Production\n",
        "# ==============================================================================\n",
        "\n",
        "@dataclass\n",
        "class QAExample:\n",
        "    \"\"\"\n",
        "    Single question-answer example with context chunks and gold answer.\n",
        "    Based on real pharmaceutical supply chain data.\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    chunks: List[str]  # All available document chunks\n",
        "    gold_answer: str\n",
        "    category: str\n",
        "\n",
        "# Real examples (anonymized) from the Moonylever deployment\n",
        "PRODUCTION_EXAMPLES = [\n",
        "    QAExample(\n",
        "        question=\"Why was Route 47 flagged in the RCA report?\",\n",
        "        chunks=[\n",
        "            \"Route Root Cause Analysis procedures require evaluation of three primary factors: vehicle condition, driver compliance, and external disruptions.\",\n",
        "            \"Driver overtime thresholds are tracked in the weekly logistics sheet. Column G records hours worked. Maximum allowable is 12 hours per shift.\",\n",
        "            \"Fuel consumption variance is calculated as (actual - baseline) / baseline. Baseline values are established quarterly during efficiency reviews.\",\n",
        "            \"Class-A restricted zones are defined in the regional compliance database. Routes intersecting these zones during peak hours require special authorization.\",\n",
        "            \"Vehicle maintenance logs must be reviewed for any route flagged in RCA. Common issues include transmission problems, brake failures, and tire wear.\",\n",
        "            \"Road closure notifications from municipal authorities are logged in the operations database. Unplanned closures require immediate route reassignment.\",\n",
        "            \"Warehouse loading delays can cascade into route delays. Staffing shortages during peak periods are the most common cause of loading issues.\",\n",
        "            \"Temperature monitoring deviations trigger automatic route flags. Cold chain integrity must be maintained for pharmaceutical cargo.\",\n",
        "        ],\n",
        "        gold_answer=\"Route 47 was flagged due to vehicle breakdown (transmission failure), unexpected road closure from municipal construction, and delayed warehouse loading caused by staffing shortages.\",\n",
        "        category=\"RCA\"\n",
        "    ),\n",
        "\n",
        "    QAExample(\n",
        "        question=\"What are the driver scheduling constraints for route assignments?\",\n",
        "        chunks=[\n",
        "            \"Driver scheduling policy (revised March 2023): Maximum 5 consecutive working days. Mandatory 2 days rest after 5-day cycle.\",\n",
        "            \"Previous policy (2021-2023): Maximum 6 consecutive working days. This policy was superseded due to fatigue-related incidents.\",\n",
        "            \"Driver availability is checked in real-time via the fleet management system. Only drivers with valid certifications can be assigned pharmaceutical routes.\",\n",
        "            \"Route assignments must account for driver experience level. New drivers (<6 months) cannot be assigned to high-value cargo routes.\",\n",
        "            \"Emergency override procedures allow scheduling beyond normal constraints with approval from operations manager and documented justification.\",\n",
        "        ],\n",
        "        gold_answer=\"Current policy allows maximum 5 consecutive working days with mandatory 2 days rest. Route assignments must verify driver certification and experience level. Emergency overrides require manager approval.\",\n",
        "        category=\"Scheduling\"\n",
        "    ),\n",
        "\n",
        "    QAExample(\n",
        "        question=\"What factors are considered in transport cost allocation?\",\n",
        "        chunks=[\n",
        "            \"Cost allocation methodology (Version 3.2, 2022): Direct costs include fuel, tolls, driver wages. Indirect costs include vehicle depreciation and insurance.\",\n",
        "            \"Fuel costs are calculated using actual consumption data from vehicle telematics. Regional fuel price indices are updated weekly.\",\n",
        "            \"Driver wage allocation uses hourly rates adjusted for overtime, night shifts, and hazard premiums for pharmaceutical cargo.\",\n",
        "            \"Vehicle depreciation follows straight-line method over 8-year useful life. Insurance costs are prorated based on route risk classification.\",\n",
        "            \"Toll expenses are captured via automated transponder systems. Manual toll submissions require photo documentation and approval.\",\n",
        "            \"Historical cost allocation (2019-2021): Used estimated fuel consumption rather than actual data. Superseded by current telematics-based method.\",\n",
        "        ],\n",
        "        gold_answer=\"Cost allocation includes direct costs (fuel, tolls, driver wages) and indirect costs (vehicle depreciation, insurance). Fuel uses actual telematics data. Driver wages account for overtime and hazard premiums. Current methodology effective 2022.\",\n",
        "        category=\"Cost\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "print(f\"âœ“ Loaded {len(PRODUCTION_EXAMPLES)} real production examples\")\n",
        "print(f\"  Categories: {', '.join(set(ex.category for ex in PRODUCTION_EXAMPLES))}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: Simple RAG System Implementation\n",
        "# ==============================================================================\n",
        "\n",
        "class SimpleRAG:\n",
        "    \"\"\"\n",
        "    Minimal RAG system for demonstration.\n",
        "    Uses Chroma for vector storage and sentence transformers for embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_model=\"all-MiniLM-L6-v2\"):\n",
        "        self.client = chromadb.Client()\n",
        "        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "            model_name=embedding_model\n",
        "        )\n",
        "        self.collection = None\n",
        "\n",
        "    def index_documents(self, chunks: List[str], collection_name: str = \"demo\"):\n",
        "        \"\"\"Index document chunks into vector store\"\"\"\n",
        "        try:\n",
        "            self.client.delete_collection(name=collection_name)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        self.collection = self.client.create_collection(\n",
        "            name=collection_name,\n",
        "            embedding_function=self.embedding_fn\n",
        "        )\n",
        "\n",
        "        self.collection.add(\n",
        "            documents=chunks,\n",
        "            ids=[f\"chunk_{i}\" for i in range(len(chunks))]\n",
        "        )\n",
        "\n",
        "        return len(chunks)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[str]:\n",
        "        \"\"\"Retrieve top-k most relevant chunks\"\"\"\n",
        "        if self.collection is None:\n",
        "            return []\n",
        "\n",
        "        results = self.collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=min(top_k, self.collection.count())\n",
        "        )\n",
        "\n",
        "        return results['documents'][0] if results['documents'] else []\n",
        "\n",
        "    def generate_answer(self, query: str, context: str, model=\"simulated\") -> str:\n",
        "        \"\"\"\n",
        "        Generate answer from context.\n",
        "\n",
        "        In production, this calls your LLM (GPT-4, Claude, etc.)\n",
        "        For this demo, we simulate generation based on context presence.\n",
        "        \"\"\"\n",
        "        # For actual implementation, replace with:\n",
        "        # response = openai.ChatCompletion.create(...)\n",
        "        # return response.choices[0].message.content\n",
        "\n",
        "        # Simulated generation for demo purposes\n",
        "        return f\"[Generated answer based on {len(context.split())} words of context]\"\n",
        "\n",
        "print(\"\\nâœ“ RAG system class defined\")\n",
        "rag = SimpleRAG()\n",
        "print(\"  Using embedding model: all-MiniLM-L6-v2\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: LLM Judge for Faithfulness and Context Recall\n",
        "# ==============================================================================\n",
        "\n",
        "class LLMJudge:\n",
        "    \"\"\"\n",
        "    LLM-as-a-judge for extracting and verifying factual statements.\n",
        "\n",
        "    In production, replace simulate_llm_call with actual API calls to GPT-4 or Claude.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def simulate_llm_call(prompt: str, task: str = \"extract\") -> str:\n",
        "        \"\"\"\n",
        "        Simulate LLM responses for demo purposes.\n",
        "\n",
        "        In production, replace with:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "        \"\"\"\n",
        "        # Simulated responses based on task type\n",
        "        if task == \"extract_answer\":\n",
        "            return \"\"\"1. Route 47 was flagged due to vehicle breakdown\n",
        "2. Transmission failure was the specific issue\n",
        "3. Road closure occurred due to municipal construction\n",
        "4. Warehouse loading delays were caused by staffing shortages\"\"\"\n",
        "\n",
        "        elif task == \"extract_gold\":\n",
        "            return \"\"\"1. Route 47 flagging was due to vehicle breakdown\n",
        "2. Breakdown was transmission failure\n",
        "3. Road closure was from municipal construction\n",
        "4. Delayed warehouse loading\n",
        "5. Staffing shortage caused loading delay\"\"\"\n",
        "\n",
        "        elif task == \"verify\":\n",
        "            # Simulate verification based on simple heuristics\n",
        "            return \"YES\" if \"vehicle\" in prompt.lower() or \"route\" in prompt.lower() else \"NO\"\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def extract_statements(self, text: str, source: str = \"answer\") -> List[str]:\n",
        "        \"\"\"Extract atomic factual statements from text using LLM\"\"\"\n",
        "        task = \"extract_answer\" if source == \"answer\" else \"extract_gold\"\n",
        "\n",
        "        prompt = f\"\"\"Extract all factual claims from the following text as a numbered list.\n",
        "Each claim should be atomic (one fact per line).\n",
        "Do not add interpretation or inference.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Provide only the numbered list of factual claims.\"\"\"\n",
        "\n",
        "        response = self.simulate_llm_call(prompt, task=task)\n",
        "\n",
        "        # Parse numbered list\n",
        "        statements = []\n",
        "        for line in response.strip().split('\\n'):\n",
        "            line = line.strip()\n",
        "            if line and (line[0].isdigit() or line.startswith('-')):\n",
        "                # Remove numbering (e.g., \"1. \" or \"- \")\n",
        "                statement = line.split('.', 1)[-1].strip()\n",
        "                if statement:\n",
        "                    statements.append(statement)\n",
        "\n",
        "        return statements\n",
        "\n",
        "    def verify_statement(self, statement: str, context: str) -> bool:\n",
        "        \"\"\"Verify if statement is supported by context using LLM\"\"\"\n",
        "        prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Statement: {statement}\n",
        "\n",
        "Is this statement directly supported by the context above?\n",
        "Consider:\n",
        "- The statement must be explicitly present or clearly inferrable\n",
        "- Paraphrasing is acceptable if meaning is preserved\n",
        "- Do not accept speculation or unsupported claims\n",
        "\n",
        "Answer only 'YES' or 'NO'.\"\"\"\n",
        "\n",
        "        response = self.simulate_llm_call(prompt, task=\"verify\")\n",
        "        return response.strip().upper() == \"YES\"\n",
        "\n",
        "print(\"\\nâœ“ LLM Judge class defined\")\n",
        "judge = LLMJudge()\n",
        "print(\"  Note: Using simulated LLM for demo. Replace with actual API calls for production.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: Faithfulness Calculation\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_faithfulness(answer: str, context: str, judge: LLMJudge,\n",
        "                          verbose: bool = False) -> Tuple[float, Dict]:\n",
        "    \"\"\"\n",
        "    Calculate Faithfulness score: F = (supported statements) / (total statements)\n",
        "\n",
        "    Faithfulness measures: \"Did the model make anything up?\"\n",
        "    - F = 1.0: Every claim is traceable to context (no hallucinations)\n",
        "    - F = 0.6: 40% of the answer is fabricated\n",
        "\n",
        "    Args:\n",
        "        answer: Generated answer from RAG system\n",
        "        context: Retrieved context (concatenated chunks)\n",
        "        judge: LLM judge instance\n",
        "        verbose: Print detailed verification results\n",
        "\n",
        "    Returns:\n",
        "        (faithfulness_score, details_dict)\n",
        "    \"\"\"\n",
        "    # Extract statements from answer\n",
        "    statements = judge.extract_statements(answer, source=\"answer\")\n",
        "\n",
        "    if not statements:\n",
        "        return 1.0, {\"statements\": 0, \"supported\": 0, \"unsupported_statements\": []}\n",
        "\n",
        "    # Verify each statement\n",
        "    supported = []\n",
        "    unsupported = []\n",
        "\n",
        "    for stmt in statements:\n",
        "        is_supported = judge.verify_statement(stmt, context)\n",
        "\n",
        "        if is_supported:\n",
        "            supported.append(stmt)\n",
        "        else:\n",
        "            unsupported.append(stmt)\n",
        "\n",
        "        if verbose:\n",
        "            status = \"âœ“\" if is_supported else \"âœ—\"\n",
        "            print(f\"  {status} {stmt[:80]}...\")\n",
        "\n",
        "    faithfulness = len(supported) / len(statements)\n",
        "\n",
        "    details = {\n",
        "        \"statements\": len(statements),\n",
        "        \"supported\": len(supported),\n",
        "        \"unsupported\": len(unsupported),\n",
        "        \"unsupported_statements\": unsupported\n",
        "    }\n",
        "\n",
        "    return faithfulness, details\n",
        "\n",
        "print(\"\\nâœ“ Faithfulness calculation function defined\")\n",
        "print(\"  Formula: F = (statements supported by context) / (total statements in answer)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 6: Context Recall Calculation\n",
        "# ==============================================================================\n",
        "\n",
        "def calculate_context_recall(gold_answer: str, context: str, judge: LLMJudge,\n",
        "                             verbose: bool = False) -> Tuple[float, Dict]:\n",
        "    \"\"\"\n",
        "    Calculate Context Recall score: R = (gold statements in context) / (total gold statements)\n",
        "\n",
        "    Context Recall measures: \"Did retrieval pull all necessary information?\"\n",
        "    - R = 1.0: Context contains everything needed for complete answer\n",
        "    - R = 0.5: Retriever only found half the required information\n",
        "\n",
        "    Args:\n",
        "        gold_answer: Reference/ground truth answer\n",
        "        context: Retrieved context (concatenated chunks)\n",
        "        judge: LLM judge instance\n",
        "        verbose: Print detailed attribution results\n",
        "\n",
        "    Returns:\n",
        "        (recall_score, details_dict)\n",
        "    \"\"\"\n",
        "    # Extract statements from gold answer\n",
        "    gold_statements = judge.extract_statements(gold_answer, source=\"gold\")\n",
        "\n",
        "    if not gold_statements:\n",
        "        return 1.0, {\"gold_statements\": 0, \"found\": 0, \"missing_statements\": []}\n",
        "\n",
        "    # Check which statements are present in context\n",
        "    found = []\n",
        "    missing = []\n",
        "\n",
        "    for stmt in gold_statements:\n",
        "        is_present = judge.verify_statement(stmt, context)\n",
        "\n",
        "        if is_present:\n",
        "            found.append(stmt)\n",
        "        else:\n",
        "            missing.append(stmt)\n",
        "\n",
        "        if verbose:\n",
        "            status = \"âœ“\" if is_present else \"âœ—\"\n",
        "            print(f\"  {status} {stmt[:80]}...\")\n",
        "\n",
        "    recall = len(found) / len(gold_statements)\n",
        "\n",
        "    details = {\n",
        "        \"gold_statements\": len(gold_statements),\n",
        "        \"found\": len(found),\n",
        "        \"missing\": len(missing),\n",
        "        \"missing_statements\": missing\n",
        "    }\n",
        "\n",
        "    return recall, details\n",
        "\n",
        "print(\"\\nâœ“ Context Recall calculation function defined\")\n",
        "print(\"  Formula: R = (gold statements found in context) / (total gold statements)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 7: The Problem - Standard Metrics Miss Hallucinations\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEMONSTRATION 1: Why Standard Metrics Fail\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Simulate the Route 47 example\n",
        "example = PRODUCTION_EXAMPLES[0]\n",
        "\n",
        "# Low K (K=1): Safe but shallow\n",
        "print(\"\\n--- Configuration: K=1 (retrieve 1 chunk) ---\")\n",
        "rag.index_documents(example.chunks, \"demo_k1\")\n",
        "retrieved_k1 = rag.retrieve(example.question, top_k=1)\n",
        "context_k1 = \"\\n\\n\".join(retrieved_k1)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_k1)} chunk(s)\")\n",
        "print(f\"Context preview: {context_k1[:150]}...\")\n",
        "\n",
        "# Simulate answer with high grounding, low completeness\n",
        "simulated_answer_k1 = \"Route RCA requires evaluation of vehicle condition, driver compliance, and external disruptions per standard procedures.\"\n",
        "\n",
        "print(f\"\\nGenerated answer: {simulated_answer_k1}\")\n",
        "\n",
        "f_k1, f_details_k1 = calculate_faithfulness(simulated_answer_k1, context_k1, judge, verbose=True)\n",
        "r_k1, r_details_k1 = calculate_context_recall(example.gold_answer, context_k1, judge, verbose=True)\n",
        "\n",
        "print(f\"\\nFaithfulness (F): {f_k1:.2f} â† High grounding\")\n",
        "print(f\"Context Recall (R): {r_k1:.2f} â† Low completeness\")\n",
        "print(f\"User Experience: Correct but useless. Missing critical specifics.\")\n",
        "\n",
        "# High K (K=7): Complete but risky\n",
        "print(\"\\n--- Configuration: K=7 (retrieve 7 chunks) ---\")\n",
        "rag.index_documents(example.chunks, \"demo_k7\")\n",
        "retrieved_k7 = rag.retrieve(example.question, top_k=7)\n",
        "context_k7 = \"\\n\\n\".join(retrieved_k7)\n",
        "\n",
        "print(f\"Retrieved {len(retrieved_k7)} chunk(s)\")\n",
        "\n",
        "# Simulate answer with fabrication due to noise\n",
        "simulated_answer_k7 = \"Route 47 was flagged due to driver overtime exceeding 12 hours (column G), fuel variance 18% above Q2 baseline, and intersection with Class-A restricted zones. Additionally, vehicle transmission issues and road closures contributed.\"\n",
        "\n",
        "print(f\"\\nGenerated answer: {simulated_answer_k7}\")\n",
        "\n",
        "f_k7, f_details_k7 = calculate_faithfulness(simulated_answer_k7, context_k7, judge, verbose=False)\n",
        "r_k7, r_details_k7 = calculate_context_recall(example.gold_answer, context_k7, judge, verbose=False)\n",
        "\n",
        "print(f\"\\nFaithfulness (F): {f_k7:.2f} â† Dropped (hallucinations)\")\n",
        "print(f\"Context Recall (R): {r_k7:.2f} â† High completeness\")\n",
        "print(f\"User Experience: Comprehensive but contains fabricated details.\")\n",
        "\n",
        "print(f\"\\nâš ï¸  The Problem: More context increased completeness but decreased grounding.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 8: Mapping the Complete Frontier\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEMONSTRATION 2: The Faithfulness-Recall Frontier\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def evaluate_configuration(example: QAExample, k: int, rag: SimpleRAG,\n",
        "                          judge: LLMJudge) -> Dict:\n",
        "    \"\"\"Evaluate RAG system at a specific K value\"\"\"\n",
        "\n",
        "    # Index and retrieve\n",
        "    rag.index_documents(example.chunks, f\"demo_k{k}\")\n",
        "    retrieved = rag.retrieve(example.question, top_k=k)\n",
        "    context = \"\\n\\n\".join(retrieved)\n",
        "\n",
        "    # Simulate generation with varying quality based on K\n",
        "    # In production, this would be actual LLM generation\n",
        "    if k <= 2:\n",
        "        # Low K: high faithfulness, low recall\n",
        "        answer = \"Basic procedural answer covering minimal facts.\"\n",
        "        f_sim, r_sim = 0.95 - (k * 0.02), 0.45 + (k * 0.08)\n",
        "    elif k <= 4:\n",
        "        # Medium K: balanced\n",
        "        answer = \"More comprehensive answer with multiple factors.\"\n",
        "        f_sim, r_sim = 0.93 - (k * 0.04), 0.55 + (k * 0.06)\n",
        "    else:\n",
        "        # High K: low faithfulness, high recall\n",
        "        answer = \"Very detailed answer incorporating many sources.\"\n",
        "        f_sim, r_sim = 0.95 - (k * 0.05), 0.60 + (k * 0.05)\n",
        "\n",
        "    # Calculate metrics\n",
        "    faithfulness, f_details = calculate_faithfulness(answer, context, judge)\n",
        "    recall, r_details = calculate_context_recall(example.gold_answer, context, judge)\n",
        "\n",
        "    # Use simulated values for consistent demo\n",
        "    faithfulness, recall = f_sim, r_sim\n",
        "\n",
        "    return {\n",
        "        'k': k,\n",
        "        'faithfulness': faithfulness,\n",
        "        'recall': recall,\n",
        "        'ratio': faithfulness / recall if recall > 0 else float('inf'),\n",
        "        'chunks_retrieved': len(retrieved)\n",
        "    }\n",
        "\n",
        "# Evaluate across K range\n",
        "k_range = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "results = []\n",
        "\n",
        "print(\"\\nEvaluating across K âˆˆ [1, 2, 3, 4, 5, 6, 7, 8]\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"{'K':<4} {'F':<6} {'R':<6} {'F/R':<6} {'Status'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "F_MIN = 0.90  # Minimum acceptable faithfulness\n",
        "\n",
        "for k in k_range:\n",
        "    result = evaluate_configuration(PRODUCTION_EXAMPLES[0], k, rag, judge)\n",
        "    results.append(result)\n",
        "\n",
        "    status = \"âœ“\" if result['faithfulness'] >= F_MIN else \"âœ—\"\n",
        "    print(f\"{result['k']:<4} {result['faithfulness']:.3f}  {result['recall']:.3f}  \"\n",
        "          f\"{result['ratio']:.3f}  {status}\")\n",
        "\n",
        "# Find optimal configuration\n",
        "valid_configs = [r for r in results if r['faithfulness'] >= F_MIN]\n",
        "\n",
        "if valid_configs:\n",
        "    optimal = max(valid_configs, key=lambda x: x['recall'])\n",
        "    print(f\"\\nðŸŽ¯ OPTIMAL: K={optimal['k']}\")\n",
        "    print(f\"   Faithfulness: {optimal['faithfulness']:.3f} (meets {F_MIN} threshold)\")\n",
        "    print(f\"   Recall: {optimal['recall']:.3f} (maximized)\")\n",
        "    print(f\"   F/R Ratio: {optimal['ratio']:.3f}\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  No configuration meets faithfulness threshold\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 9: Visualization\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"VISUALIZATION: The F-R Trade-off\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot 1: Faithfulness vs Recall frontier\n",
        "f_vals = [r['faithfulness'] for r in results]\n",
        "r_vals = [r['recall'] for r in results]\n",
        "k_vals = [r['k'] for r in results]\n",
        "\n",
        "# Scatter plot with color gradient\n",
        "scatter = ax1.scatter(r_vals, f_vals, c=k_vals, cmap='viridis',\n",
        "                     s=150, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
        "ax1.plot(r_vals, f_vals, 'k--', alpha=0.3, linewidth=1)\n",
        "\n",
        "# Annotate points\n",
        "for i, k in enumerate(k_vals):\n",
        "    ax1.annotate(f'K={k}', (r_vals[i], f_vals[i]),\n",
        "                xytext=(8, 0), textcoords='offset points',\n",
        "                fontsize=9, fontweight='bold')\n",
        "\n",
        "# Mark optimal point\n",
        "if valid_configs:\n",
        "    optimal_r = optimal['recall']\n",
        "    optimal_f = optimal['faithfulness']\n",
        "    ax1.scatter([optimal_r], [optimal_f], s=400, marker='*',\n",
        "               c='red', edgecolors='black', linewidth=2.5,\n",
        "               label=f'Optimal (K={optimal[\"k\"]})', zorder=10)\n",
        "\n",
        "# Threshold line\n",
        "ax1.axhline(y=F_MIN, color='red', linestyle='--', linewidth=2,\n",
        "           alpha=0.6, label=f'F_min = {F_MIN}')\n",
        "\n",
        "# Styling\n",
        "ax1.set_xlabel('Context Recall (R)', fontsize=13, fontweight='bold')\n",
        "ax1.set_ylabel('Faithfulness (F)', fontsize=13, fontweight='bold')\n",
        "ax1.set_title('The Optimal Frontier: Grounding vs Completeness',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3, linestyle=':')\n",
        "ax1.legend(fontsize=10, loc='lower right')\n",
        "ax1.set_xlim(0.35, 1.0)\n",
        "ax1.set_ylim(0.55, 1.0)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(scatter, ax=ax1)\n",
        "cbar.set_label('Top-K', fontsize=11, fontweight='bold')\n",
        "\n",
        "# Plot 2: F/R Ratio across K\n",
        "ratios = [r['ratio'] for r in results]\n",
        "\n",
        "ax2.plot(k_vals, ratios, 'o-', linewidth=2.5, markersize=10,\n",
        "        color='steelblue', markeredgecolor='black', markeredgewidth=1.5)\n",
        "\n",
        "if valid_configs:\n",
        "    ax2.scatter([optimal['k']], [optimal['ratio']], s=400, marker='*',\n",
        "               c='red', edgecolors='black', linewidth=2.5, zorder=10,\n",
        "               label='Optimal')\n",
        "\n",
        "# Styling\n",
        "ax2.set_xlabel('Top-K', fontsize=13, fontweight='bold')\n",
        "ax2.set_ylabel('F/R Ratio (lower = better)', fontsize=13, fontweight='bold')\n",
        "ax2.set_title('Efficiency: Grounding per Unit Completeness',\n",
        "             fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, linestyle=':')\n",
        "ax2.set_xticks(k_vals)\n",
        "ax2.legend(fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('rag_faithfulness_recall_frontier.png', dpi=300, bbox_inches='tight')\n",
        "print(\"\\nâœ“ Visualization saved: rag_faithfulness_recall_frontier.png\")\n",
        "plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 10: Production Implementation Guide\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRODUCTION IMPLEMENTATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "production_code = '''\n",
        "# Step 1: Replace simulated LLM with actual API calls\n",
        "\n",
        "import openai  # or import anthropic\n",
        "\n",
        "def extract_statements_production(text: str, api_key: str) -> List[str]:\n",
        "    \"\"\"Extract factual statements using GPT-4\"\"\"\n",
        "\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Extract all factual claims from this text as a numbered list.\n",
        "Each claim should be atomic (one fact per line).\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Provide only the numbered list.\"\"\"\n",
        "        }],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    # Parse response\n",
        "    statements = []\n",
        "    for line in response.choices[0].message.content.split('\\\\n'):\n",
        "        line = line.strip()\n",
        "        if line and line[0].isdigit():\n",
        "            statement = line.split('.', 1)[-1].strip()\n",
        "            if statement:\n",
        "                statements.append(statement)\n",
        "\n",
        "    return statements\n",
        "\n",
        "\n",
        "def verify_statement_production(statement: str, context: str, api_key: str) -> bool:\n",
        "    \"\"\"Verify statement support using GPT-4\"\"\"\n",
        "\n",
        "    client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Context: {context}\n",
        "\n",
        "Statement: {statement}\n",
        "\n",
        "Is this statement directly supported by the context?\n",
        "Answer only YES or NO.\"\"\"\n",
        "        }],\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip().upper() == \"YES\"\n",
        "\n",
        "\n",
        "# Step 2: Evaluate your RAG system\n",
        "\n",
        "def evaluate_rag_production(questions: List[str],\n",
        "                           gold_answers: List[str],\n",
        "                           your_rag_system,\n",
        "                           k_values: List[int],\n",
        "                           api_key: str):\n",
        "    \"\"\"Complete evaluation across K configurations\"\"\"\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for k in k_values:\n",
        "        f_scores = []\n",
        "        r_scores = []\n",
        "\n",
        "        for q, gold in zip(questions, gold_answers):\n",
        "            # Your RAG system\n",
        "            context = your_rag_system.retrieve(q, top_k=k)\n",
        "            answer = your_rag_system.generate(q, context)\n",
        "\n",
        "            # Calculate metrics\n",
        "            answer_stmts = extract_statements_production(answer, api_key)\n",
        "            gold_stmts = extract_statements_production(gold, api_key)\n",
        "\n",
        "            # Faithfulness\n",
        "            supported = sum(1 for s in answer_stmts\n",
        "                          if verify_statement_production(s, context, api_key))\n",
        "            f = supported / len(answer_stmts) if answer_stmts else 1.0\n",
        "\n",
        "            # Context Recall\n",
        "            found = sum(1 for s in gold_stmts\n",
        "                       if verify_statement_production(s, context, api_key))\n",
        "            r = found / len(gold_stmts) if gold_stmts else 1.0\n",
        "\n",
        "            f_scores.append(f)\n",
        "            r_scores.append(r)\n",
        "\n",
        "        results.append({\n",
        "            'k': k,\n",
        "            'faithfulness': np.mean(f_scores),\n",
        "            'recall': np.mean(r_scores)\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Step 3: Find optimal configuration\n",
        "\n",
        "def find_optimal_k(results: List[Dict], f_min: float = 0.90) -> int:\n",
        "    \"\"\"Find K that maximizes recall while maintaining faithfulness >= f_min\"\"\"\n",
        "\n",
        "    valid = [r for r in results if r['faithfulness'] >= f_min]\n",
        "\n",
        "    if not valid:\n",
        "        print(f\"Warning: No configuration meets f_min={f_min}\")\n",
        "        return results[0]['k']\n",
        "\n",
        "    optimal = max(valid, key=lambda x: x['recall'])\n",
        "    return optimal['k']\n",
        "'''\n",
        "\n",
        "print(production_code)\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 11: Key Takeaways\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY TAKEAWAYS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "1. Standard Metrics Are Misleading\n",
        "   - ROUGE, BLEU, and Answer F1 measure textual similarity, not truth\n",
        "   - High scores can coexist with severe hallucinations\n",
        "   - These metrics cannot isolate retrieval vs generation failures\n",
        "\n",
        "2. Two Dimensions Matter\n",
        "   - Faithfulness (F): Proportion of answer statements supported by context\n",
        "   - Context Recall (R): Proportion of gold answer present in context\n",
        "   - These are orthogonal: you can fail on one while succeeding on the other\n",
        "\n",
        "3. The Trade-off Is Fundamental\n",
        "   - Increasing Top-K increases recall but decreases faithfulness\n",
        "   - The relationship is non-linear and monotonic\n",
        "   - You cannot maximize both simultaneously with current architectures\n",
        "\n",
        "4. The Strategy Is Mathematical\n",
        "   - Define minimum acceptable faithfulness (F_min) based on use case\n",
        "   - Maximize recall subject to F >= F_min constraint\n",
        "   - Track F/R ratio to measure grounding efficiency\n",
        "\n",
        "5. Production Requirements\n",
        "   - Replace simulated LLM with actual API calls (GPT-4, Claude)\n",
        "   - Evaluate on domain-specific data with expert-validated gold answers\n",
        "   - Monitor both metrics in production, not just end-to-end accuracy\n",
        "   - Set alerts for faithfulness drops below threshold\n",
        "\n",
        "6. The Next Frontier\n",
        "   - Current RAG architectures force the F-R trade-off\n",
        "   - Future systems must break this: increase R without sacrificing F\n",
        "   - Possible directions: graph RAG, multi-hop retrieval, agentic reasoning\n",
        "   - But these introduce new failure modes that need measurement\n",
        "\n",
        "7. What Changed For Me\n",
        "   - Before: Trusted ROUGE scores, deployed broken system\n",
        "   - After: Measure F and R, understand true performance\n",
        "   - The shift: From optimizing similarity to optimizing truth\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"For the complete mathematical treatment and real production narrative:\")\n",
        "print(\"[Link to Substack Article]\")\n",
        "print(\"=\"*80)"
      ]
    }
  ]
}